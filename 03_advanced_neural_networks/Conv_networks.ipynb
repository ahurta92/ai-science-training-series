{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:49.366318211Z",
     "start_time": "2024-02-22T21:15:47.560283565Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc907c456d5a66d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:49.382845540Z",
     "start_time": "2024-02-22T21:15:49.379378091Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.transforms import v2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6418e67cee10648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:50.483216869Z",
     "start_time": "2024-02-22T21:15:49.379520325Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrianhurtado/anaconda3/envs/ai-training/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToTensor(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomResizedCrop(size=32, scale=(0.85, 1.0), antialias=False),\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ])\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# split the training data into training and validation\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [.8, .2], generator=torch.Generator().manual_seed(42))\n",
    "batch_size = 128\n",
    "\n",
    "# Create the dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(training_data,\n",
    "                              batch_size=batch_size,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True,\n",
    "                              num_workers=4)\n",
    "validation_dataloader = DataLoader(validation_data,\n",
    "                                   batch_size=batch_size,\n",
    "                                   pin_memory=True,  # copy the data to the GPU\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=4)  # number of subprocesses to use for data loading\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1ae569cf589332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.085870399Z",
     "start_time": "2024-02-22T21:15:50.483567162Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch, (X, Y) = next(enumerate(train_dataloader))\n",
    "plt.imshow(X[0].permute(1, 2, 0));\n",
    "plt.title(Y[0].item());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c78289d71a5d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.086426714Z",
     "start_time": "2024-02-22T21:15:51.083335957Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a2af455b60f988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.100515378Z",
     "start_time": "2024-02-22T21:15:51.086144959Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(X, Y):\n",
    "    # CIFAR-10 is *color*\n",
    "    return X.view(-1, 3, 32, 32).to(device), Y.to(device)\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "\n",
    "train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n",
    "validation_dataloader = WrappedDataLoader(validation_dataloader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7989a22c1e4e7864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.133065213Z",
     "start_time": "2024-02-22T21:15:51.105871866Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Downsampler(nn.Module):\n",
    "    # down sampler downsamples the channels.  \n",
    "    # since it's defined with kernel_size = stride, this will reduce \n",
    "    # the size of the image by stride times in each dimension.\n",
    "    # This is a simple way to downsample the image.\n",
    "    # Think of max pooling as a more sophisticated way to downsample the image.\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, shape, stride=2):\n",
    "        super(Downsampler, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        self.downsample = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=stride,\n",
    "            stride=stride,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.downsample(self.norm(inputs))\n",
    "\n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"This block of operations is loosely based on this paper:\n",
    "    start with an input with a 7 by 7 kernel, however with same padding it will not change the size of the image.\n",
    "    Then we apply a normalization layer, and follow with 4 filters, aka patches,per channel.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, shape):\n",
    "        super(ConvNextBlock, self).__init__()\n",
    "        # Depthwise, seperable convolution with a large number of output filters:\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=in_channels,\n",
    "                               groups=in_channels,\n",
    "                               kernel_size=[7, 7],\n",
    "                               padding='same')\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        # Two more convolutions:\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=4 * in_channels,\n",
    "                               kernel_size=1)\n",
    "        # Then we do the unrolling of the patches, and apply a non-linear activation function.\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=4 * in_channels,\n",
    "                               out_channels=in_channels,\n",
    "                               kernel_size=1\n",
    "                               )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        # I add another non-linear activation layer here.\n",
    "        # The normalization layer:\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # The non-linear activation layer:\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        # I add another non-linear activation layer here.\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        # and followed with a normalization layer.\n",
    "\n",
    "        # This makes it a residual network:\n",
    "        return x + inputs\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_initial_filters, n_stages, blocks_per_stage):\n",
    "        super(Classifier, self).__init__()\n",
    "        # our first example is set with 64,4,2 meaning 64 initial filters, 4 stages, 2 blocks per stage\n",
    "\n",
    "        # This is a downsampling convolution that will produce patches of output.\n",
    "\n",
    "        # This is similar to what vision transformers do to tokenize the images.\n",
    "        self.stem = nn.Conv2d(in_channels=3,\n",
    "                              out_channels=n_initial_filters,\n",
    "                              kernel_size=1,\n",
    "                              stride=1)\n",
    "        # Here is the initial convolution which takes our 32 x 32 image with 3 channels and \n",
    "        # converts it to 64 channels with a 1x1 convolution\n",
    "\n",
    "        # In other words, this stem is a patching operation that converts the input image into a set of patches.\n",
    "        # 3 to 64.  maybe here it makes sense to make n_intial_filters = 3 * filter_per_channel.. say 32\n",
    "\n",
    "        current_shape = [32, 32]\n",
    "        self.norm1 = nn.LayerNorm([n_initial_filters, *current_shape])\n",
    "        # self.norm1 = WrappedLayerNorm()\n",
    "        current_n_filters = n_initial_filters\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        for i, n_blocks in enumerate(range(n_stages)):\n",
    "            # Add a convnext block series:\n",
    "            for _ in range(blocks_per_stage):\n",
    "                self.layers.append(ConvNextBlock(in_channels=current_n_filters, shape=current_shape))\n",
    "            # Add a downsampling layer:\n",
    "            if i != n_stages - 1:\n",
    "                # Skip downsampling if it's the last layer!\n",
    "                self.layers.append(Downsampler(\n",
    "                    in_channels=current_n_filters,\n",
    "                    out_channels=2 * current_n_filters,\n",
    "                    shape=current_shape,\n",
    "                )\n",
    "                )\n",
    "                # Double the number of filters:\n",
    "                current_n_filters = 2 * current_n_filters\n",
    "                # Cut the shape in half:\n",
    "                current_shape = [cs // 2 for cs in current_shape]\n",
    "                # here w eare doing a downsampling operation, which is a patching operation.\n",
    "                # We record the current shape\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(current_n_filters),\n",
    "            nn.Linear(current_n_filters, 10)\n",
    "        )\n",
    "        # self.norm2 = nn.InstanceNorm2d(current_n_filters)\n",
    "        # # This brings it down to one channel / class\n",
    "        # self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, \n",
    "        #                                   kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.stem(inputs)\n",
    "        # Apply a normalization after the initial patching:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Apply the main chunk of the network:\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # Normalize and readout:\n",
    "        x = nn.functional.avg_pool2d(x, x.shape[2:])\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e5aa0a927e005eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.147616024Z",
     "start_time": "2024-02-22T21:15:51.112035319Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (stem): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (norm1): LayerNorm((64, 32, 32), eps=1e-05, elementwise_affine=True)\n",
       "  (layers): Sequential(\n",
       "    (0): ConvNextBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=64)\n",
       "      (norm): LayerNorm((64, 32, 32), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): ConvNextBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=64)\n",
       "      (norm): LayerNorm((64, 32, 32), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ConvNextBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=64)\n",
       "      (norm): LayerNorm((64, 32, 32), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (3): Downsampler(\n",
       "      (norm): LayerNorm((64, 32, 32), eps=1e-05, elementwise_affine=True)\n",
       "      (downsample): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (4): ConvNextBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=128)\n",
       "      (norm): LayerNorm((128, 16, 16), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (5): ConvNextBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=128)\n",
       "      (norm): LayerNorm((128, 16, 16), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (6): ConvNextBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=128)\n",
       "      (norm): LayerNorm((128, 16, 16), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (7): Downsampler(\n",
       "      (norm): LayerNorm((128, 16, 16), eps=1e-05, elementwise_affine=True)\n",
       "      (downsample): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (8): ConvNextBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=256)\n",
       "      (norm): LayerNorm((256, 8, 8), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (9): ConvNextBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=256)\n",
       "      (norm): LayerNorm((256, 8, 8), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (10): ConvNextBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=same, groups=256)\n",
       "      (norm): LayerNorm((256, 8, 8), eps=1e-05, elementwise_affine=True)\n",
       "      (conv2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Classifier(64, 4, 2)\n",
    "#model.cuda()\n",
    "# The first test will be to set the initial filter to a multiple of . Actually, Why don't I just convert the image to a 1 channel if color should not matter for these images.  I can then normalize the image as well.  \n",
    "# 3 stages \n",
    "# 2 blocks per stage\n",
    "model = Classifier(64, 3, 3)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10776dbedcca0cab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.197492835Z",
     "start_time": "2024-02-22T21:15:51.146627206Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_fn, val_bar):\n",
    "    # Set the model to evaluation mode \n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calcualting the gradients\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, Y).item()\n",
    "            correct += torch.sum(torch.argmax(pred, dim=1) == Y).item()\n",
    "            val_bar.update()\n",
    "    loss /= num_batches\n",
    "    correct /= (size * batch_size)\n",
    "    accuracy = 100 * correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2a7d2f40dc0a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.198199270Z",
     "start_time": "2024-02-22T21:15:51.191451146Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        l = loss_fn(pred, Y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4976e7c1d475a92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.198599609Z",
     "start_time": "2024-02-22T21:15:51.191599221Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a42a98bcc0551337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T21:15:51.198921429Z",
     "start_time": "2024-02-22T21:15:51.191651698Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a50c08bd5881718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:12:59.559466513Z",
     "start_time": "2024-02-22T21:15:51.191688740Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: 100%|██████████| 313/313 [01:17<00:00,  4.04it/s]\n",
      "Validate (train) Epoch 0: 100%|██████████| 313/313 [00:26<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 1.636, accuracy: 38.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 0: 100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: validation loss: 1.636, accuracy: 37.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 1: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss: 1.279, accuracy: 54.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 1: 100%|██████████| 79/79 [00:06<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: validation loss: 1.306, accuracy: 52.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 2: 100%|██████████| 313/313 [00:26<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: training loss: 1.064, accuracy: 62.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 2: 100%|██████████| 79/79 [00:06<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: validation loss: 1.102, accuracy: 59.424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 3: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: training loss: 0.847, accuracy: 70.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 3: 100%|██████████| 79/79 [00:06<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: validation loss: 0.912, accuracy: 67.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 4: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: training loss: 0.718, accuracy: 74.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 4: 100%|██████████| 79/79 [00:06<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: validation loss: 0.813, accuracy: 70.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 5: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: training loss: 0.587, accuracy: 79.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 5: 100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: validation loss: 0.700, accuracy: 74.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 6: 100%|██████████| 313/313 [00:26<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: training loss: 0.522, accuracy: 81.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 6: 100%|██████████| 79/79 [00:06<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: validation loss: 0.656, accuracy: 75.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 7: 100%|██████████| 313/313 [00:26<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: training loss: 0.428, accuracy: 85.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 7: 100%|██████████| 79/79 [00:06<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: validation loss: 0.606, accuracy: 77.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 8: 100%|██████████| 313/313 [00:26<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: training loss: 0.380, accuracy: 86.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 8: 100%|██████████| 79/79 [00:06<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: validation loss: 0.594, accuracy: 78.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 9: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: training loss: 0.336, accuracy: 88.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 9: 100%|██████████| 79/79 [00:06<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: validation loss: 0.546, accuracy: 80.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 10: 100%|██████████| 313/313 [00:26<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: training loss: 0.298, accuracy: 89.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 10: 100%|██████████| 79/79 [00:06<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: validation loss: 0.576, accuracy: 80.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 11: 100%|██████████| 313/313 [00:26<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: training loss: 0.272, accuracy: 90.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 11: 100%|██████████| 79/79 [00:06<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: validation loss: 0.585, accuracy: 80.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 12: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: training loss: 0.250, accuracy: 91.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 12: 100%|██████████| 79/79 [00:06<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: validation loss: 0.585, accuracy: 79.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 13: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: training loss: 0.211, accuracy: 92.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 13: 100%|██████████| 79/79 [00:06<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: validation loss: 0.587, accuracy: 80.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 14: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: training loss: 0.232, accuracy: 91.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 14: 100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: validation loss: 0.591, accuracy: 80.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 15: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: training loss: 0.190, accuracy: 93.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 15: 100%|██████████| 79/79 [00:06<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: validation loss: 0.582, accuracy: 80.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 16: 100%|██████████| 313/313 [00:26<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: training loss: 0.142, accuracy: 95.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 16: 100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: validation loss: 0.525, accuracy: 82.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 17: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: training loss: 0.142, accuracy: 95.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 17: 100%|██████████| 79/79 [00:06<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: validation loss: 0.571, accuracy: 81.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 18: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: training loss: 0.138, accuracy: 95.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 18: 100%|██████████| 79/79 [00:06<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: validation loss: 0.582, accuracy: 81.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 19: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: training loss: 0.137, accuracy: 95.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 19: 100%|██████████| 79/79 [00:06<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: validation loss: 0.605, accuracy: 81.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 20: 100%|██████████| 313/313 [00:26<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss: 0.106, accuracy: 96.169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 20: 100%|██████████| 79/79 [00:06<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: validation loss: 0.575, accuracy: 82.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 21: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: training loss: 0.102, accuracy: 96.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 21: 100%|██████████| 79/79 [00:06<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: validation loss: 0.555, accuracy: 83.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 22: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: training loss: 0.126, accuracy: 95.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 22: 100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: validation loss: 0.628, accuracy: 81.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 23: 100%|██████████| 313/313 [00:26<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: training loss: 0.093, accuracy: 96.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 23: 100%|██████████| 79/79 [00:06<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: validation loss: 0.581, accuracy: 82.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 24: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: training loss: 0.087, accuracy: 96.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 24: 100%|██████████| 79/79 [00:06<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: validation loss: 0.567, accuracy: 82.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 25: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: training loss: 0.105, accuracy: 96.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 25: 100%|██████████| 79/79 [00:06<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: validation loss: 0.606, accuracy: 82.427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 26: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: training loss: 0.105, accuracy: 96.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 26: 100%|██████████| 79/79 [00:06<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: validation loss: 0.634, accuracy: 82.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 27: 100%|██████████| 313/313 [00:26<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: training loss: 0.094, accuracy: 96.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 27: 100%|██████████| 79/79 [00:06<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: validation loss: 0.620, accuracy: 82.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s]\n",
      "Validate (train) Epoch 28: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: training loss: 0.083, accuracy: 97.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 28: 100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: validation loss: 0.617, accuracy: 82.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s]\n",
      "Validate (train) Epoch 29: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: training loss: 0.090, accuracy: 96.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate Epoch 29: 100%|██████████| 79/79 [00:06<00:00, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: validation loss: 0.642, accuracy: 82.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for j in range(epochs):\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n",
    "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n",
    "\n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n",
    "        acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n",
    "\n",
    "        print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n",
    "    with tqdm(total=len(validation_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "        acc_val, loss_val = evaluate(validation_dataloader, model, loss_fn, val_bar)\n",
    "        print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3e79b6b343512be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:12:59.891325461Z",
     "start_time": "2024-02-22T22:12:59.580576774Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 22 17:12:59 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    Off | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   84C    P0              27W /  45W |   3246MiB /  4096MiB |     52%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A     26618      C   ...aconda3/envs/ai-training/bin/python      134MiB |\r\n",
      "|    0   N/A  N/A     94500      C   ...aconda3/envs/ai-training/bin/python     3104MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b8ee8af9b1e3df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:12:59.939794576Z",
     "start_time": "2024-02-22T22:12:59.895494283Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea295f519af3b0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:12:59.940555766Z",
     "start_time": "2024-02-22T22:12:59.939454420Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
